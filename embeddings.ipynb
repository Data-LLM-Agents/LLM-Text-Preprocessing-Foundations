{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies if not already installed\n",
                "!pip install torch tiktoken"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LLM Text Preprocessing Foundations (Embeddings)\n",
                "\n",
                "**Nombre:** Juan Pablo Nieto Cortes  \n",
                "**Materia:** AREP  \n",
                "\n",
                "This notebook covers the core concepts of text preprocessing for Large Language Models, specifically focusing on data loading, tokenization, and embeddings. It corresponds to Chapter 2 of \"Build a Large Language Model From Scratch\"."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import tiktoken\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "\n",
                "print(\"PyTorch version:\", torch.__version__)\n",
                "print(\"tiktoken version:\", tiktoken.__version__)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading\n",
                "We load the text file `the-verdict.txt` which serves as our corpus."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
                "    raw_text = f.read()\n",
                "    \n",
                "print(\"Total characters:\", len(raw_text))\n",
                "print(raw_text[:100])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Tokenization and Data Loader\n",
                "We use `tiktoken` (BPE) for tokenization and implement a custom Dataset class to create sliding window chunks for training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class GPTDatasetV1(Dataset):\n",
                "    def __init__(self, txt, tokenizer, max_length, stride):\n",
                "        self.input_ids = []\n",
                "        self.target_ids = []\n",
                "\n",
                "        # Tokenize the entire text\n",
                "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
                "        \n",
                "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
                "        for i in range(0, len(token_ids) - max_length, stride):\n",
                "            input_chunk = token_ids[i:i + max_length]\n",
                "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
                "            self.input_ids.append(torch.tensor(input_chunk))\n",
                "            self.target_ids.append(torch.tensor(target_chunk))\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.input_ids)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        return self.input_ids[idx], self.target_ids[idx]\n",
                "\n",
                "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
                "                         stride=128, shuffle=True, drop_last=True,\n",
                "                         num_workers=0):\n",
                "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
                "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
                "    dataloader = DataLoader(\n",
                "        dataset,\n",
                "        batch_size=batch_size,\n",
                "        shuffle=shuffle,\n",
                "        drop_last=drop_last,\n",
                "        num_workers=num_workers\n",
                "    )\n",
                "    return dataloader"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Embeddings\n",
                "The core components of the LLM input layer: Token Embeddings and Positional Embeddings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vocab_size = 50257\n",
                "output_dim = 256\n",
                "max_length = 4\n",
                "batch_size = 8\n",
                "\n",
                "# Create data loader\n",
                "dataloader = create_dataloader_v1(\n",
                "    raw_text, batch_size=batch_size, max_length=max_length,\n",
                "    stride=max_length, shuffle=False\n",
                ")\n",
                "data_iter = iter(dataloader)\n",
                "inputs, targets = next(data_iter)\n",
                "print(\"Input batch shape:\", inputs.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Token Embeddings\n",
                "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
                "token_embeddings = token_embedding_layer(inputs)\n",
                "print(\"Token embeddings shape:\", token_embeddings.shape)\n",
                "\n",
                "# 2. Positional Embeddings\n",
                "pos_embedding_layer = torch.nn.Embedding(max_length, output_dim)\n",
                "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
                "print(\"Positional embeddings shape:\", pos_embeddings.shape)\n",
                "\n",
                "# 3. Input Embeddings (Token + Positional)\n",
                "input_embeddings = token_embeddings + pos_embeddings\n",
                "print(\"Final input embeddings shape:\", input_embeddings.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Explanations\n",
                "\n",
                "### Why do embeddings encode meaning?\n",
                "Embeddings encode meaning by mapping discrete tokens (like words or subwords) to continuous vectors in a high-dimensional space. During training, the model adjusts these vectors so that tokens appearing in similar contexts (e.g., \"dog\" and \"cat\" appearing near \"pet\" or \"fur\") end up close to each other in the vector space. This geometric proximity captures semantic relationships, allowing the model to understand that \"queen\" is to \"woman\" as \"king\" is to \"man\".\n",
                "\n",
                "### How are embeddings related to Neural Network concepts?\n",
                "An embedding layer is fundamentally a **learnable weight matrix** (essentially a lookup table) that serves as the first layer of the neural network. Unlike traditional manual feature extraction (like bag-of-words), embeddings are parameters that are optimized via **backpropagation** along with the rest of the network. Each row in the weight matrix corresponds to a token in the vocabulary, and these weights are updated to minimize the model's loss function.\n",
                "\n",
                "### Why do we need Positional Embeddings?\n",
                "Transformer architectures (like GPT) use self-attention mechanisms that are processed in parallel and are permutation-invariantâ€”they don't inherently know the order of tokens. Without positional information, \"The dog bit the man\" and \"The man bit the dog\" would look identical to the self-attention layer. Positional embeddings inject unique signals (vectors) for each position (1st, 2nd, 3rd...), allowing the model to distinguish the sequence order and structure.\n",
                "\n",
                "### Why is overlap (stride < max_length) useful in data loading?\n",
                "Using a stride smaller than the max_length creates overlapping chunks of text. This acts as a form of **data augmentation**, allowing the model to see the same tokens in slightly different context windows. It maximizes the usage of limited training data and helps the model learn to predict tokens based on varying amounts of preceding context, improving its generalization capabilities."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Experiment: max_length and stride\n",
                "\n",
                "We will vary `max_length` and `stride` to see how it affects the number of training samples generated from our text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_experiment(txt, max_length_vals, stride_vals):\n",
                "    print(f\"{'Max Length':<12} {'Stride':<10} {'Num Batches':<12} {'Total Samples':<12}\")\n",
                "    print(\"-\" * 50)\n",
                "    \n",
                "    for ml in max_length_vals:\n",
                "        for st in stride_vals:\n",
                "            # Create dataloader with batch_size=1 to count all samples easily\n",
                "            dataloader = create_dataloader_v1(\n",
                "                txt, batch_size=1, max_length=ml, stride=st, shuffle=False, drop_last=False\n",
                "            )\n",
                "            num_batches = len(dataloader)\n",
                "            print(f\"{ml:<12} {st:<10} {num_batches:<12} {num_batches:<12}\")\n",
                "\n",
                "# Experiment parameters\n",
                "max_lengths = [4, 10, 50]\n",
                "strides = [1, 2, 4, 10, 50]\n",
                "\n",
                "run_experiment(raw_text, max_lengths, strides)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Observation:**\n",
                "- Smaller `stride` results in significantly more samples (more overlap).\n",
                "- Larger `stride` (e.g., equal to `max_length`) results in fewer samples with no overlap.\n",
                "- `max_length` also constraints the number of samples as each sample must be at least that long.\n",
                "\n",
                "As explained above, overlap is useful for maximizing the utility of a small dataset."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}